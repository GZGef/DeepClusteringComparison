# –ü—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–∞

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

### 1. –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

```bash
# –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–≥–æ –æ–∫—Ä—É–∂–µ–Ω–∏—è (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# –∏–ª–∏
venv\Scripts\activate  # Windows

# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π
pip install -r requirements.txt
```

### 2. –ó–∞–ø—É—Å–∫ –ø—Ä–æ–µ–∫—Ç–∞

```bash
python main.py
```

### 3. –û–∂–∏–¥–∞–µ–º—ã–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç

–ü—Ä–æ–µ–∫—Ç –≤—ã–ø–æ–ª–Ω–∏—Ç —Å–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:
1. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö Mall_Customers
2. –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ (3 –º–µ—Ç–æ–¥–∞)
3. –ü—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞ (500 —ç–ø–æ—Ö)
4. –û–±—É—á–µ–Ω–∏–µ DEC –º–æ–¥–µ–ª–∏ (–¥–æ 250 —ç–ø–æ—Ö)
5. –û–±—É—á–µ–Ω–∏–µ K-means
6. –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
7. –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ (8 –≥—Ä–∞—Ñ–∏–∫–æ–≤)

## üìä –ü—Ä–∏–º–µ—Ä—ã –≤—ã–≤–æ–¥–∞

### –ú–µ—Ç—Ä–∏–∫–∏ –∫–∞—á–µ—Å—Ç–≤–∞

```
=== –ú–ï–¢–†–ò–ö–ò –ö–ê–ß–ï–°–¢–í–ê: K-Means ===
–°–∏–ª—É—ç—Ç–Ω—ã–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç: 0.4523

=== –ú–ï–¢–†–ò–ö–ò –ö–ê–ß–ï–°–¢–í–ê: DEC ===
–°–∏–ª—É—ç—Ç–Ω—ã–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç: 0.5217

=== –°–†–ê–í–ù–ï–ù–ò–ï –ú–ï–¢–û–î–û–í ===
Homogeneity: 0.6542
Completeness: 0.6789
V-Measure: 0.6663
```

### –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º (K-Means)

```
=== –°–¢–ê–¢–ò–°–¢–ò–ö–ê –ü–û –ö–õ–ê–°–¢–ï–†–ê–ú: K-Means ===

–ö–ª–∞—Å—Ç–µ—Ä 0:
  –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ—á–µ–∫: 45
  –°—Ä–µ–¥–Ω–∏–π Annual Income: $45.23k
  –°—Ä–µ–¥–Ω–∏–π Age: 32.5 –ª–µ—Ç
  –°—Ä–µ–¥–Ω–∏–π Spending Score: 52.3

–ö–ª–∞—Å—Ç–µ—Ä 1:
  –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ—á–µ–∫: 38
  –°—Ä–µ–¥–Ω–∏–π Annual Income: $78.45k
  –°—Ä–µ–¥–Ω–∏–π Age: 45.2 –ª–µ—Ç
  –°—Ä–µ–¥–Ω–∏–π Spending Score: 42.1
```

## üéØ –ü—Ä–∏–º–µ—Ä—ã –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏–∏ –ø—Ä–æ–µ–∫—Ç–∞

### –ò–∑–º–µ–Ω–µ–Ω–∏–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤

```python
# –í main.py –∏–∑–º–µ–Ω–∏—Ç–µ n_clusters
n_clusters = 5  # –í–º–µ—Å—Ç–æ 11
```

### –ò–∑–º–µ–Ω–µ–Ω–∏–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –æ–±—É—á–µ–Ω–∏—è

```python
# –í main.py –∏–∑–º–µ–Ω–∏—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
ae_loss, ae_loss_history = train_autoencoder(
    autoencoder=autoencoder,
    dataloader=dataloader,
    device=device,
    epochs=300,  # –ú–µ–Ω—å—à–µ —ç–ø–æ—Ö
    learning_rate=1e-4  # –î—Ä—É–≥–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
)

dec_loss, dec_loss_history, dec_shift_history = train_dec(
    dec_model=dec_model,
    dataloader=dataloader,
    device=device,
    epochs=150,  # –ú–µ–Ω—å—à–µ —ç–ø–æ—Ö
    learning_rate=1e-3  # –î—Ä—É–≥–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å –æ–±—É—á–µ–Ω–∏—è
)
```

### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –¥—Ä—É–≥–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞

```python
# –í src/data_loader.py –∏–∑–º–µ–Ω–∏—Ç–µ —Ñ—É–Ω–∫—Ü–∏—é load_and_preprocess_data
def load_and_preprocess_data(url: str = None, file_path: str = 'your_dataset.csv') -> tuple:
    # –ó–∞–≥—Ä—É–∑–∫–∞ –≤–∞—à–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
    df = pd.read_csv(file_path, sep=',')
    
    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞
    # ...
    
    return df_normalized, df, feature_names
```

### –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –Ω–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫

```python
# –í src/evaluation.py –¥–æ–±–∞–≤—å—Ç–µ –Ω–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score

def calculate_metrics(kmeans_labels, dec_labels, data):
    metrics = {}
    
    # –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
    metrics['kmeans'] = evaluate_clustering(data, kmeans_labels, "K-Means")
    metrics['dec'] = evaluate_clustering(data, dec_labels, "DEC")
    
    # –ù–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    if len(np.unique(kmeans_labels)) == len(np.unique(dec_labels)):
        ari = adjusted_rand_score(kmeans_labels, dec_labels)
        nmi = normalized_mutual_info_score(kmeans_labels, dec_labels)
        
        metrics['comparison']['adjusted_rand_index'] = ari
        metrics['comparison']['normalized_mutual_info'] = nmi
    
    return metrics
```

## üîß –û—Ç–ª–∞–¥–∫–∞ –∏ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞

### –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU

```python
import torch
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"CUDA devices: {torch.cuda.device_count()}")
if torch.cuda.is_available():
    print(f"Current device: {torch.cuda.current_device()}")
    print(f"Device name: {torch.cuda.get_device_name(0)}")
```

### –£–º–µ–Ω—å—à–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è

–î–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–∂–Ω–æ —É–º–µ–Ω—å—à–∏—Ç—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö:

```python
# –í main.py
ae_loss, ae_loss_history = train_autoencoder(
    # ...
    epochs=50,  # –í–º–µ—Å—Ç–æ 500
)

dec_loss, dec_loss_history, dec_shift_history = train_dec(
    # ...
    epochs=25,  # –í–º–µ—Å—Ç–æ 250
)
```

### –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π

```python
# –í main.py –ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è
import torch

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞–≤—Ç–æ—ç–Ω–∫–æ–¥–µ—Ä–∞
torch.save(autoencoder.state_dict(), 'autoencoder.pth')

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ DEC –º–æ–¥–µ–ª–∏
torch.save({
    'model_state_dict': dec_model.state_dict(),
    'cluster_centers': dec_model.cluster_centers,
}, 'dec_model.pth')

# –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π
autoencoder = Autoencoder(input_dim=4, hidden_dim=16, latent_dim=2)
autoencoder.load_state_dict(torch.load('autoencoder.pth'))

dec_model = DEC(autoencoder, n_clusters=11, latent_dim=2, alpha=1.0)
checkpoint = torch.load('dec_model.pth')
dec_model.load_state_dict(checkpoint['model_state_dict'])
dec_model.cluster_centers = checkpoint['cluster_centers']
```

## üìà –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –¥—Ä—É–≥–∏–º–∏ –º–µ—Ç–æ–¥–∞–º–∏

### DBSCAN (–¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è)

```python
from sklearn.cluster import DBSCAN

# –î–æ–±–∞–≤—å—Ç–µ –≤ main.py
dbscan = DBSCAN(eps=0.5, min_samples=5)
dbscan_labels = dbscan.fit_predict(df_normalized)

# –û—Ü–µ–Ω–∫–∞ (–µ—Å–ª–∏ –µ—Å—Ç—å —à—É–º–æ–≤—ã–µ —Ç–æ—á–∫–∏)
unique_labels = np.unique(dbscan_labels)
if -1 in unique_labels:
    # –£–¥–∞–ª–µ–Ω–∏–µ —à—É–º–∞ –¥–ª—è –æ—Ü–µ–Ω–∫–∏
    mask = dbscan_labels != -1
    if np.sum(mask) > 1:
        dbscan_metrics = evaluate_clustering(
            df_normalized[mask], 
            dbscan_labels[mask], 
            "DBSCAN"
        )
```

### Agglomerative Clustering

```python
from sklearn.cluster import AgglomerativeClustering

# –î–æ–±–∞–≤—å—Ç–µ –≤ main.py
agg = AgglomerativeClustering(n_clusters=11)
agg_labels = agg.fit_predict(df_normalized)

agg_metrics = evaluate_clustering(df_normalized, agg_labels, "Agglomerative")
```

## üéì –û–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã–µ –ø—Ä–∏–º–µ—Ä—ã

### –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤

```python
# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –ª–∞—Ç–µ–Ω—Ç–Ω–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞
import matplotlib.pyplot as plt

# –ü–æ–ª—É—á–µ–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
with torch.no_grad():
    z, _ = autoencoder(df_tensor.to(device))
    z = z.cpu().numpy()

# 2D –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è
plt.figure(figsize=(10, 8))
plt.scatter(z[:, 0], z[:, 1], alpha=0.6, s=50)
plt.title('Latent Space Visualization')
plt.xlabel('Latent Dim 1')
plt.ylabel('Latent Dim 2')
plt.grid(True, alpha=0.3)
plt.savefig('latent_space.png', dpi=300)
plt.show()
```

### –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫

```python
# –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è
with torch.no_grad():
    z, x_recon = autoencoder(df_tensor.to(device))
    errors = torch.mean((df_tensor.to(device) - x_recon) ** 2, dim=1).cpu().numpy()

# –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –æ—à–∏–±–æ–∫
plt.figure(figsize=(12, 6))
plt.hist(errors, bins=50, edgecolor='black', alpha=0.7)
plt.title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è')
plt.xlabel('MSE Error')
plt.ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
plt.grid(True, alpha=0.3)
plt.savefig('reconstruction_errors.png', dpi=300)
plt.show()
```

## üìö –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã

### –ü–æ–ª–µ–∑–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã

```bash
# –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏
python -c "import torch; print(torch.__version__)"
python -c "import sklearn; print(sklearn.__version__)"

# –ó–∞–ø—É—Å–∫ —Å –æ—Ç–ª–∞–¥–∫–æ–π
python -m pdb main.py
```

### –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –≤ Jupyter

```python
# –í Jupyter Notebook
%matplotlib inline
import matplotlib.pyplot as plt
import numpy as np

# –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–æ–≤
from IPython.display import Image
Image('elbow_method.png')
```

## üéØ –ß–µ–∫-–ª–∏—Å—Ç –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º

- [ ] –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –≤—Å–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ (`pip install -r requirements.txt`)
- [ ] –î–æ—Å—Ç—É–ø–µ–Ω –¥–∞—Ç–∞—Å–µ—Ç Mall_Customers.csv (–∏–ª–∏ —Å–∫—Ä–∏–ø—Ç –º–æ–∂–µ—Ç –µ–≥–æ —Å–∫–∞—á–∞—Ç—å)
- [ ] –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ –º–µ—Å—Ç–∞ –Ω–∞ –¥–∏—Å–∫–µ (–¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –≥—Ä–∞—Ñ–∏–∫–æ–≤)
- [ ] Python 3.8+ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
- [ ] PyTorch –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω (–ø—Ä–æ–≤–µ—Ä–∫–∞: `import torch; print(torch.cuda.is_available())`)
- [ ] –í—Å–µ —Ñ–∞–π–ª—ã –ø—Ä–æ–µ–∫—Ç–∞ –Ω–∞ –º–µ—Å—Ç–µ

## üêõ –ò–∑–≤–µ—Å—Ç–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã –∏ —Ä–µ—à–µ–Ω–∏—è

### –ü—Ä–æ–±–ª–µ–º–∞: "No module named 'torchinfo'"

**–†–µ—à–µ–Ω–∏–µ:**
```bash
pip install torchinfo
```

### –ü—Ä–æ–±–ª–µ–º–∞: "CUDA out of memory"

**–†–µ—à–µ–Ω–∏–µ:**
- –£–º–µ–Ω—å—à–∏—Ç–µ batch_size –≤ main.py
- –£–º–µ–Ω—å—à–∏—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö
- –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ CPU: `device = torch.device('cpu')`

### –ü—Ä–æ–±–ª–µ–º–∞: "Dataset not found"

**–†–µ—à–µ–Ω–∏–µ:**
- –°–∫–∞—á–∞–π—Ç–µ –¥–∞—Ç–∞—Å–µ—Ç –≤—Ä—É—á–Ω—É—é: `wget https://storage.yandexcloud.net/google-colab-bucket/Mall_Customers.csv`
- –ò–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ª–æ–∫–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª

### –ü—Ä–æ–±–ª–µ–º–∞: "Matplotlib backend error"

**–†–µ—à–µ–Ω–∏–µ:**
```python
import matplotlib
matplotlib.use('Agg')  # –î–æ–±–∞–≤—å—Ç–µ –≤ –Ω–∞—á–∞–ª–æ main.py
```

## üìû –ü–æ–¥–¥–µ—Ä–∂–∫–∞

–ï—Å–ª–∏ –≤–æ–∑–Ω–∏–∫–ª–∏ –ø—Ä–æ–±–ª–µ–º—ã:
1. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏ –≤—ã–≤–æ–¥–∞ –≤ –∫–æ–Ω—Å–æ–ª–∏
2. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤—Å–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã
3. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –Ω–∞–ª–∏—á–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
4. –£–º–µ–Ω—å—à–∏—Ç–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è

## üéì –î–∞–ª—å–Ω–µ–π—à–µ–µ –∏–∑—É—á–µ–Ω–∏–µ

### –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:
1. –ò–∑—É—á–∏—Ç–µ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É DEC –≤ —Å—Ç–∞—Ç—å–µ "Unsupervised Deep Embedded Clustering"
2. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø—Ä–∏–º–µ–Ω–∏—Ç—å –º–µ—Ç–æ–¥ –∫ –¥—Ä—É–≥–∏–º –¥–∞—Ç–∞—Å–µ—Ç–∞–º
3. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —Å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
4. –î–æ–±–∞–≤—å—Ç–µ –Ω–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏
5. –†–µ–∞–ª–∏–∑—É–π—Ç–µ –¥—Ä—É–≥–∏–µ –≥–ª—É–±–æ–∫–∏–µ –º–µ—Ç–æ–¥—ã –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏

### –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è –ª–∏—Ç–µ—Ä–∞—Ç—É—Ä–∞:
- "Deep Learning" by Ian Goodfellow (–≥–ª—É–±–æ–∫–æ–µ –æ–±—É—á–µ–Ω–∏–µ)
- "Pattern Recognition and Machine Learning" by Christopher Bishop (–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è)
- "Unsupervised Deep Embedded Clustering" by Xie et al. (DEC)